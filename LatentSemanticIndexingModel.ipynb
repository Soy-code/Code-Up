{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPgfmJZqqYwOflMUQqPgoCY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soy-code/Code-Up/blob/master/LatentSemanticIndexingModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7gB0-KvsOGO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 잠재의미분석(LSA/LSI)\n",
        "- DTM의 잠재된(Latent) 의미를 이끌어내는 방법\n",
        "- 기존의 DTM이나 DTM에 단어의 중요도에 따른 가중치를 주었던 TF-IDF 행렬은 단어의 의미를 전혀 고려하지 못한다는 단점이 있음\n",
        "- LSA는 DTM이나 TF-IDF 행렬에 truncated SVD 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어낸다는 아이디어\n"
      ],
      "metadata": {
        "id": "D3FE41NjtOn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 실습\n",
        "import numpy as np\n",
        "A = np.array([[0,0,0,1,0,1,1,0,0],\n",
        "              [0,0,0,1,1,0,1,0,0],\n",
        "              [0,1,1,0,2,0,0,0,0],\n",
        "              [1,0,0,0,0,0,0,1,1]])\n",
        "print(A.shape)\n",
        "\n",
        "# full SVD\n",
        "U, s, VT = np.linalg.svd(A, full_matrices = True) # U : 직교행렬, s : 대각행렬, VT: V의 전치행렬\n",
        "print(U.shape, s.shape, VT.shape)\n",
        "print(U.round(2))\n",
        "print(s.round(2))\n",
        "print(VT.round(2))\n",
        "\n",
        "# s를 대각행렬로 만들기\n",
        "S = np.zeros((4, 9))\n",
        "S[:4, :4] = np.diag(s)\n",
        "print(S.round(2))"
      ],
      "metadata": {
        "id": "TUU9r9cHtdDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncated SVD\n",
        "t = 2\n",
        "S = S[:t, :t]\n",
        "print(S.round(2))\n",
        "\n",
        "U = U[:, :t]\n",
        "print(U.round(2))\n",
        "\n",
        "VT = VT[:t, :]\n",
        "print(VT.round(2))"
      ],
      "metadata": {
        "id": "KC7RWZ3BuHkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_prime =np.dot(np.dot(U, S), VT)\n",
        "print(A)\n",
        "print(A_prime.round(2))"
      ],
      "metadata": {
        "id": "zlhKx0A3uCL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 뉴스그룹 예시\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "metadata": {
        "id": "BVBHc9upwksc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = fetch_20newsgroups(shuffle= True, random_state = 1,\n",
        "                             remove = ('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data"
      ],
      "metadata": {
        "id": "dngd4k-xxl-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df = pd.DataFrame({'document': documents})\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "ZP11J10ezA7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())  # 토큰화\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
      ],
      "metadata": {
        "id": "psLxpZSXzFZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf-idf 행렬 만들기\n",
        "detokenized_doc = []\n",
        "for i in range(len(news_df)) :\n",
        "  t = ' '.join(tokenized_doc[i])\n",
        "  detokenized_doc.append(t)\n",
        "\n",
        "news_df['clean_doc'] = detokenized_doc"
      ],
      "metadata": {
        "id": "dLs1IwHD11db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 1000,\n",
        "                             max_df = 0.5, smooth_idf =True)\n",
        "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "ZL-XFtz62V_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토픽 모델링\n",
        "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
        "svd_model.fit(X)\n",
        "len(svd_model.components_)"
      ],
      "metadata": {
        "id": "HqCs9nEc47fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terms = vectorizer.get_feature_names_out() # 단어 집합. 1,000개의 단어가 저장됨.\n",
        "\n",
        "def get_topics(components, feature_names, n=5):\n",
        "    for idx, topic in enumerate(components):\n",
        "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
        "get_topics(svd_model.components_,terms)"
      ],
      "metadata": {
        "id": "Rccu9A5t5RQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AO6UN4e_5Oug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gensim모델"
      ],
      "metadata": {
        "id": "lo1GbG6L6A1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "k0XRd5ks6EYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import preprocess_documents\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "0WUNCndE6R7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/betacom/latent-semantic-indexing-in-python-85880414b4de\n",
        "\n",
        "https://www.projectpro.io/recipes/create-lsi-topic-model-gensim\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/"
      ],
      "metadata": {
        "id": "BSulbR086YJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv"
      ],
      "metadata": {
        "id": "Yad-GOCkfzW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/sample_data/wiki_movie_plots_deduped.csv', encoding = 'utf8')"
      ],
      "metadata": {
        "id": "F7eZn3Stf7E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = csv.reader(f)\n",
        "csv_list = []\n",
        "for l in reader :\n",
        "  csv_list.append(l)\n",
        "f.close()\n",
        "\n",
        "df = pd.DataFrame(csv_list)"
      ],
      "metadata": {
        "id": "cC0bxD1Rf-V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.iloc[0]\n",
        "df = df.drop(0)\n",
        "df.reset_index(drop = True, inplace = True)"
      ],
      "metadata": {
        "id": "P9zAnjm9gTzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "i6JG1wTOgRKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Release Year'] = df['Release Year'].astype(int)"
      ],
      "metadata": {
        "id": "nEI8XRaRgytj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['Release Year'] >= 2000]\n",
        "new_df = df[df['Release Year'] < 2000]\n",
        "text_corpus =df['Plot'].values"
      ],
      "metadata": {
        "id": "IvOXew-ygNmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_corpus = preprocess_documents(text_corpus)\n",
        "dictionary = gensim.corpora.Dictionary(processed_corpus)\n",
        "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]"
      ],
      "metadata": {
        "id": "EMQmSGQShBBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = gensim.models.TfidfModel(bow_corpus, smartirs = 'npu')\n",
        "corpus_tfidf = tfidf[bow_corpus]"
      ],
      "metadata": {
        "id": "aWtie6DrhLm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lsi = gensim.models.LsiModel(corpus_tfidf, num_topics=200)\n",
        "index = gensim.similarities.MatrixSimilarity(lsi[corpus_tfidf])"
      ],
      "metadata": {
        "id": "xv4lbUBRhTwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "csS7OBRGkce4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_doc = gensim.parsing.preprocessing.preprocess_string(new_doc)\n",
        "new_vec = dictionary.doc2bow(new_doc)\n",
        "vec_bow_tfidf = tfidf[new_vec]\n",
        "vec_lsi = lsi[vec_bow_tfidf]\n",
        "sims = index[vec_lsi]\n",
        "for s in sorted(enumerate(sims), key=lambda item: -item[1])[:10]:\n",
        "    print(f”{df[‘Title’].iloc[s[0]]} : {str(s[1])}”)"
      ],
      "metadata": {
        "id": "waAfvvWAhwZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Gensim is a powerful library for natural language processing in Python.\",\n",
        "    \"LSI model can be used for topic modeling in Gensim.\",\n",
        "    \"Topic modeling is useful to discover hidden semantic patterns in text data.\"\n",
        "]\n",
        "\n",
        "tokenized_documents = [word_tokenize(doc.lower()) for doc in documents]\n",
        "dictionary = gensim.corpora.Dictionary(tokenized_documents)\n",
        "corpus = [dictionary.doc2bow(tokenized_doc) for tokenized_doc in tokenized_documents]\n",
        "lsi_model = gensim.models.LsiModel(corpus, id2word=dictionary, num_topics= 1)\n",
        "topics = lsi_model.print_topics()\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "metadata": {
        "id": "Nnkdz-Sa6vQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Find_Optimal_Cutoff(target, predicted):\n",
        "      \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n",
        "      Parameters\n",
        "      ----------\n",
        "      target : Matrix with dependent or target data, where rows are observations\n",
        "      predicted : Matrix with predicted data, where rows are observations\n",
        "      Returns\n",
        "      -------\n",
        "      list type, with optimal cutoff value\n",
        "      \"\"\"\n",
        "      fpr, tpr, threshold = roc_curve(target, predicted)\n",
        "      i = np.arange(len(tpr))\n",
        "      roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
        "      roc_t = roc.ix[(roc.tf-0).abs().argsort()[:1]]\n",
        "\n",
        "      return list(roc_t['threshold'])"
      ],
      "metadata": {
        "id": "IWy9KjUObMJz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}